{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aadc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = r\"/home/abhayadana/Downloads/SummerLowStatureSampling_pt_2025.csv\"\n",
    "OUTPUT_DIR = r\"/home/abhayadana/Documents/GitHub/SLSS_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9640c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SIS workflow per strManagementUnit with diagnostics bundle and robust p_max/winner\n",
    "computation (no All-NaN slice warnings).\n",
    "\n",
    "Requested diagnostics per unit:\n",
    "1) variogram_J_fit.png\n",
    "2) cv_scores.csv + cv_scores.png\n",
    "4) variogram_fit_surface_<best_model>.png\n",
    "5) realizations_panel.png\n",
    "\n",
    "Fix applied:\n",
    "- Replace np.nanmax / np.nanargmax on prob_stack with a safe max/argmax that\n",
    "  handles outside-domain all-NaN slices without warnings.\n",
    "\n",
    "Also added:\n",
    "- Verbose per-unit printout of class counts (n) and percent of unit total.\n",
    "\n",
    "CRS assumption:\n",
    "- UTM Zone 10, NAD83, meters (EPSG:26910)\n",
    "\n",
    "Dependencies:\n",
    "- numpy, pandas, rasterio, matplotlib\n",
    "Optional:\n",
    "- shapely (envelope buffering)\n",
    "- scipy (ndimage MMU cleanup; ConvexHull fallback)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm, ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# User parameters (edit in your notebook)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "INPUT_CSV = r\"/home/abhayadana/Downloads/SummerLowStatureSampling_pt_2025.csv\"\n",
    "OUTPUT_DIR = r\"/home/abhayadana/Documents/GitHub/SLSS_analysis\"\n",
    "CRS_EPSG = 26910\n",
    "\n",
    "# Grid\n",
    "GRID_CELL_SIZE_M = 1.0\n",
    "EXTENT_BUFFER_M = 0.0\n",
    "\n",
    "# Envelope domain (convex hull)\n",
    "USE_POINT_ENVELOPE_MASK = True\n",
    "ENVELOPE_BUFFER_M = 5.0\n",
    "\n",
    "# Declustering\n",
    "DECLUSTER_MULT = 2.0\n",
    "DECLUSTER_CELL_MIN_M = 5.0\n",
    "DECLUSTER_CELL_MAX_M = 50.0\n",
    "\n",
    "# Spatial block CV (for variogram family selection on J)\n",
    "CV_FOLDS = 5\n",
    "BLOCK_SIZE_MULT = 10.0\n",
    "BLOCK_SIZE_MIN_M = 25.0\n",
    "BLOCK_SIZE_MAX_M = 100.0\n",
    "\n",
    "# Variogram fitting\n",
    "N_LAGS = 12\n",
    "MAX_DIST_FRACTION_OF_SPAN = 0.5\n",
    "MAX_PAIRS_FOR_VARIOGRAM = 200_000\n",
    "RANGE_GRID_SIZE = 18\n",
    "NUGGET_GRID_SIZE = 10\n",
    "NUGGET_MAX = 0.30\n",
    "\n",
    "# Neighborhood for local kriging in CV and SIS\n",
    "SEARCH_RADIUS_FACTOR = 1.5\n",
    "MIN_NEIGHBORS = 8\n",
    "MAX_NEIGHBORS = 40\n",
    "\n",
    "# SIS settings\n",
    "N_REALIZATIONS = 10\n",
    "RANDOM_SEED = 57718\n",
    "\n",
    "# Outputs / post-processing\n",
    "UNCERTAIN_TOLERANCE = 0.50\n",
    "MMU_CELLS = 2\n",
    "\n",
    "# Rare-class stabilization (optional)\n",
    "SILL_FLOOR = 0.0  # set to 0.005 or 0.01 if needed\n",
    "\n",
    "# Realization panel\n",
    "PANEL_N = 9  # number of realizations to show in panel (e.g., 9 -> 3x3)\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Classes (fixed order)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "CLASSES = [\n",
    "    \"Short Open\",\n",
    "    \"Tall Open\",\n",
    "    \"Mid Mod\",\n",
    "    \"Short Dense\",\n",
    "    \"Tall Dense\",\n",
    "]\n",
    "CLASS_TO_CODE = {cls: i + 1 for i, cls in enumerate(CLASSES)}\n",
    "CODE_TO_CLASS = {0: \"Uncertain\", **{i + 1: cls for i, cls in enumerate(CLASSES)}}\n",
    "\n",
    "# 0 is transparent/blank in PNGs; 1..5 are class colors\n",
    "PALETTE = [\n",
    "    \"#00000000\",  # 0 Uncertain/Masked (transparent)\n",
    "    \"#1f77b4\",    # 1 Short Open\n",
    "    \"#ff7f0e\",    # 2 Tall Open\n",
    "    \"#2ca02c\",    # 3 Mid Mod\n",
    "    \"#d62728\",    # 4 Short Dense\n",
    "    \"#9467bd\",    # 5 Tall Dense\n",
    "]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data structures\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UnitData:\n",
    "    unit: str\n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "    code: np.ndarray\n",
    "    present_codes: List[int]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VariogramModel:\n",
    "    model_type: str  # \"exponential\" | \"spherical\" | \"gaussian\"\n",
    "    range_m: float\n",
    "    nugget: float\n",
    "    sill_total: float  # total sill for J\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CondPoint:\n",
    "    x: float\n",
    "    y: float\n",
    "    code: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VariogramFitArtifacts:\n",
    "    lag: np.ndarray\n",
    "    gamma_emp: np.ndarray\n",
    "    bin_w: np.ndarray\n",
    "    sill_total: float\n",
    "    range_grid: np.ndarray\n",
    "    nugget_grid: np.ndarray\n",
    "    sse_grid: np.ndarray  # shape (len(nugget_grid), len(range_grid))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Small utilities\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def safe_name(name: str) -> str:\n",
    "    \"\"\"Make a safe folder/file stem from a string.\"\"\"\n",
    "    name = str(name).strip()\n",
    "    name = re.sub(r\"[^\\w\\-]+\", \"_\", name)\n",
    "    name = re.sub(r\"_+\", \"_\", name).strip(\"_\")\n",
    "    return name or \"unit\"\n",
    "\n",
    "\n",
    "def nearest_neighbor_distances(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return per-point nearest-neighbor distances (meters).\n",
    "    Uses scipy.cKDTree if available; falls back to sklearn.\n",
    "    \"\"\"\n",
    "    pts = np.column_stack([x, y]).astype(float)\n",
    "    if pts.shape[0] < 2:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "    try:\n",
    "        from scipy.spatial import cKDTree  # type: ignore\n",
    "\n",
    "        tree = cKDTree(pts)\n",
    "        dists, _ = tree.query(pts, k=2)  # self + nearest\n",
    "        return dists[:, 1].astype(float)\n",
    "    except Exception:\n",
    "        try:\n",
    "            from sklearn.neighbors import NearestNeighbors  # type: ignore\n",
    "\n",
    "            nn = NearestNeighbors(n_neighbors=2).fit(pts)\n",
    "            dists, _ = nn.kneighbors(pts, n_neighbors=2)\n",
    "            return dists[:, 1].astype(float)\n",
    "        except Exception:\n",
    "            # last-resort fallback\n",
    "            return np.array([], dtype=float)\n",
    "\n",
    "\n",
    "def print_unit_class_summary(unit_data: UnitData) -> None:\n",
    "    \"\"\"\n",
    "    Verbose per-unit printout of:\n",
    "      - class counts and percent of unit total\n",
    "      - nearest-neighbor distance summary stats (meters)\n",
    "      - derived declustering cell size and block size (meters)\n",
    "    \"\"\"\n",
    "    n = int(len(unit_data.code))\n",
    "    print(f\"Unit: {unit_data.unit}  |  n={n}\")\n",
    "\n",
    "    # Class counts\n",
    "    counts = {c: int(np.sum(unit_data.code == c)) for c in range(1, 6)}\n",
    "    for c in range(1, 6):\n",
    "        pct = 100.0 * counts[c] / max(n, 1)\n",
    "        print(f\"  - {CODE_TO_CLASS[c]}: n={counts[c]} ({pct:.1f}%)\")\n",
    "\n",
    "    present = [CODE_TO_CLASS[c] for c in unit_data.present_codes]\n",
    "    print(f\"  Present classes: {present}\")\n",
    "\n",
    "    # Nearest-neighbor distance stats\n",
    "    nn = nearest_neighbor_distances(unit_data.x, unit_data.y)\n",
    "    if nn.size == 0:\n",
    "        print(\"  NN distance (m): n/a (need >=2 points and scipy or sklearn)\")\n",
    "        print(\"  Declustering cell (m): n/a\")\n",
    "        print(\"  Block size (m): n/a\")\n",
    "        return\n",
    "\n",
    "    q10, q50, q90 = np.percentile(nn, [10, 50, 90])\n",
    "    nn_min = float(np.min(nn))\n",
    "    nn_med = float(q50)\n",
    "    nn_mean = float(np.mean(nn))\n",
    "    nn_max = float(np.max(nn))\n",
    "\n",
    "    print(\n",
    "        \"  NN distance (m): \"\n",
    "        f\"min={nn_min:.2f}, p10={q10:.2f}, median={nn_med:.2f}, \"\n",
    "        f\"mean={nn_mean:.2f}, p90={q90:.2f}, max={nn_max:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Derived sizes from median NN (with clamps)\n",
    "    decluster_cell = float(np.clip(DECLUSTER_MULT * nn_med, DECLUSTER_CELL_MIN_M, DECLUSTER_CELL_MAX_M))\n",
    "    block_size = float(np.clip(BLOCK_SIZE_MULT * nn_med, BLOCK_SIZE_MIN_M, BLOCK_SIZE_MAX_M))\n",
    "\n",
    "    print(\n",
    "        \"  Declustering cell (m): \"\n",
    "        f\"{decluster_cell:.2f}  (mult={DECLUSTER_MULT:g}, clamp=[{DECLUSTER_CELL_MIN_M:g},{DECLUSTER_CELL_MAX_M:g}])\"\n",
    "    )\n",
    "    print(\n",
    "        \"  Block size (m): \"\n",
    "        f\"{block_size:.2f}  (mult={BLOCK_SIZE_MULT:g}, clamp=[{BLOCK_SIZE_MIN_M:g},{BLOCK_SIZE_MAX_M:g}])\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def safe_max_and_argmax(prob_stack: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Safe computation of p_max and winner for a probability stack with NaNs.\n",
    "\n",
    "    Returns:\n",
    "      p_max (float32): max probability, NaN where all classes are NaN\n",
    "      winner (uint8): 1..K winner, 0 where all classes are NaN\n",
    "      all_nan (bool): mask where all classes are NaN\n",
    "    \"\"\"\n",
    "    finite = np.isfinite(prob_stack)\n",
    "    all_nan = np.all(~finite, axis=2)\n",
    "    prob_for_ops = np.where(finite, prob_stack, -np.inf)\n",
    "\n",
    "    p_max = np.max(prob_for_ops, axis=2).astype(np.float32)\n",
    "    p_max[all_nan] = np.nan\n",
    "\n",
    "    winner = (np.argmax(prob_for_ops, axis=2) + 1).astype(np.uint8)\n",
    "    winner[all_nan] = 0\n",
    "\n",
    "    return p_max, winner, all_nan\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# IO helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def read_points(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV and validate required fields.\"\"\"\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    required = {\"strManagementUnit\", \"strAveHeight_cm_PctCover\", \"UTM_X\", \"UTM_Y\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df[\"UTM_X\"] = pd.to_numeric(df[\"UTM_X\"], errors=\"coerce\")\n",
    "    df[\"UTM_Y\"] = pd.to_numeric(df[\"UTM_Y\"], errors=\"coerce\")\n",
    "    df[\"strAveHeight_cm_PctCover\"] = df[\"strAveHeight_cm_PctCover\"].astype(str).str.strip()\n",
    "\n",
    "    df = df.dropna(subset=[\"UTM_X\", \"UTM_Y\", \"strManagementUnit\", \"strAveHeight_cm_PctCover\"]).copy()\n",
    "    df = df[df[\"strAveHeight_cm_PctCover\"].isin(CLASSES)].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid points after filtering to known classes and valid coordinates.\")\n",
    "\n",
    "    df[\"class_code\"] = df[\"strAveHeight_cm_PctCover\"].map(CLASS_TO_CODE).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_by_unit(df: pd.DataFrame) -> List[UnitData]:\n",
    "    \"\"\"Split dataframe into per-unit UnitData objects.\"\"\"\n",
    "    units: List[UnitData] = []\n",
    "    for unit, g in df.groupby(\"strManagementUnit\", sort=True):\n",
    "        x = g[\"UTM_X\"].to_numpy(float)\n",
    "        y = g[\"UTM_Y\"].to_numpy(float)\n",
    "        code = g[\"class_code\"].to_numpy(int)\n",
    "        present = sorted(np.unique(code).tolist())\n",
    "        units.append(UnitData(unit=str(unit), x=x, y=y, code=code, present_codes=present))\n",
    "    return units\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Grid + envelope domain\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def unit_bbox(unit_data: UnitData, buffer_m: float = 0.0) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Axis-aligned bbox of points, used to create raster transform.\"\"\"\n",
    "    xmin = float(np.min(unit_data.x)) - float(buffer_m)\n",
    "    ymin = float(np.min(unit_data.y)) - float(buffer_m)\n",
    "    xmax = float(np.max(unit_data.x)) + float(buffer_m)\n",
    "    ymax = float(np.max(unit_data.y)) + float(buffer_m)\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "\n",
    "def build_grid(\n",
    "    xmin: float,\n",
    "    ymin: float,\n",
    "    xmax: float,\n",
    "    ymax: float,\n",
    "    cell_size: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, rasterio.Affine]:\n",
    "    \"\"\"Build a raster grid with cell centers and a rasterio transform.\"\"\"\n",
    "    width = int(math.ceil((xmax - xmin) / cell_size))\n",
    "    height = int(math.ceil((ymax - ymin) / cell_size))\n",
    "\n",
    "    transform = from_origin(xmin, ymax, cell_size, cell_size)\n",
    "    x_centers = xmin + (np.arange(width) + 0.5) * cell_size\n",
    "    y_centers = ymax - (np.arange(height) + 0.5) * cell_size\n",
    "    return x_centers, y_centers, transform\n",
    "\n",
    "\n",
    "def build_point_envelope_geometry(x: np.ndarray, y: np.ndarray, buffer_m: float = 0.0) -> Sequence[dict]:\n",
    "    \"\"\"\n",
    "    Build a convex hull geometry around points, optionally buffered.\n",
    "\n",
    "    Returns GeoJSON-like geometry dict(s) suitable for rasterio.geometry_mask.\n",
    "    \"\"\"\n",
    "    pts = np.column_stack([x, y]).astype(float)\n",
    "\n",
    "    try:\n",
    "        from shapely.geometry import MultiPoint  # type: ignore\n",
    "        from shapely.geometry import mapping  # type: ignore\n",
    "\n",
    "        hull = MultiPoint(pts).convex_hull\n",
    "        if buffer_m and float(buffer_m) != 0.0:\n",
    "            hull = hull.buffer(float(buffer_m))\n",
    "        return [mapping(hull)]\n",
    "    except Exception:\n",
    "        try:\n",
    "            from scipy.spatial import ConvexHull  # type: ignore\n",
    "        except Exception as exc:\n",
    "            raise ImportError(\"Envelope masking requires shapely (preferred) or scipy.\") from exc\n",
    "\n",
    "        hull = ConvexHull(pts)\n",
    "        ring = pts[hull.vertices].tolist()\n",
    "        ring.append(ring[0])\n",
    "        return [{\"type\": \"Polygon\", \"coordinates\": [ring]}]\n",
    "\n",
    "\n",
    "def envelope_mask_for_grid(\n",
    "    geoms: Sequence[dict],\n",
    "    out_shape: Tuple[int, int],\n",
    "    transform: rasterio.Affine,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Boolean mask of pixels inside envelope geometry. True = inside domain.\"\"\"\n",
    "    inside = geometry_mask(\n",
    "        geometries=geoms,\n",
    "        out_shape=out_shape,\n",
    "        transform=transform,\n",
    "        invert=True,\n",
    "        all_touched=False,\n",
    "    )\n",
    "    return inside\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Nearest neighbor distance (median)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def estimate_median_nn_distance(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Median nearest-neighbor distance using KD-tree if available.\"\"\"\n",
    "    pts = np.column_stack([x, y]).astype(float)\n",
    "    if pts.shape[0] < 2:\n",
    "        return 1.0\n",
    "\n",
    "    try:\n",
    "        from scipy.spatial import cKDTree  # type: ignore\n",
    "\n",
    "        tree = cKDTree(pts)\n",
    "        dists, _ = tree.query(pts, k=2)\n",
    "        return float(np.median(dists[:, 1]))\n",
    "    except Exception:\n",
    "        try:\n",
    "            from sklearn.neighbors import NearestNeighbors  # type: ignore\n",
    "\n",
    "            nn = NearestNeighbors(n_neighbors=2).fit(pts)\n",
    "            dists, _ = nn.kneighbors(pts, n_neighbors=2)\n",
    "            return float(np.median(dists[:, 1]))\n",
    "        except Exception:\n",
    "            dx = pts[:, 0].max() - pts[:, 0].min()\n",
    "            dy = pts[:, 1].max() - pts[:, 1].min()\n",
    "            return float(max(1.0, 0.01 * math.hypot(dx, dy)))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Declustering weights\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def compute_declustering_weights(x: np.ndarray, y: np.ndarray, cell_size_m: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cell declustering weights: w_i = 1 / n_cell(i), normalized to mean 1.0.\n",
    "    \"\"\"\n",
    "    xmin = float(np.min(x))\n",
    "    ymin = float(np.min(y))\n",
    "\n",
    "    cx = np.floor((x - xmin) / cell_size_m).astype(int)\n",
    "    cy = np.floor((y - ymin) / cell_size_m).astype(int)\n",
    "    key = cx.astype(np.int64) * 1_000_000 + cy.astype(np.int64)\n",
    "\n",
    "    _, inv, counts = np.unique(key, return_inverse=True, return_counts=True)\n",
    "    w = 1.0 / counts[inv].astype(float)\n",
    "    w *= (len(w) / max(w.sum(), EPS))\n",
    "    return w\n",
    "\n",
    "\n",
    "def weighted_prevalence(code: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Weighted prevalence p_k for each code 1..5. Returns shape (5,), sums to 1.\"\"\"\n",
    "    p = np.zeros(5, dtype=float)\n",
    "    wsum = float(np.sum(weights))\n",
    "    if wsum <= 0:\n",
    "        return np.ones(5, dtype=float) / 5.0\n",
    "\n",
    "    for k in range(1, 6):\n",
    "        p[k - 1] = float(np.sum(weights[code == k]) / wsum)\n",
    "\n",
    "    s = float(np.sum(p))\n",
    "    return p / max(s, EPS)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spatial blocks for CV\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def assign_blocks(x: np.ndarray, y: np.ndarray, block_size_m: float) -> np.ndarray:\n",
    "    \"\"\"Assign points to spatial blocks using a regular grid.\"\"\"\n",
    "    xmin = float(np.min(x))\n",
    "    ymin = float(np.min(y))\n",
    "    bx = np.floor((x - xmin) / block_size_m).astype(int)\n",
    "    by = np.floor((y - ymin) / block_size_m).astype(int)\n",
    "    return bx.astype(np.int64) * 1_000_000 + by.astype(np.int64)\n",
    "\n",
    "\n",
    "def make_block_folds(block_ids: np.ndarray, n_folds: int, seed: int) -> List[np.ndarray]:\n",
    "    \"\"\"Return list of boolean masks for test points in each fold.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    uniq = np.unique(block_ids)\n",
    "    rng.shuffle(uniq)\n",
    "    parts = np.array_split(uniq, n_folds)\n",
    "    return [np.isin(block_ids, blk) for blk in parts]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Variogram models and covariance\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def variogram_gamma(model_type: str, h: np.ndarray, rng_m: float, nugget: float, partial_sill: float) -> np.ndarray:\n",
    "    \"\"\"Semivariogram γ(h) for exp/sph/gau.\"\"\"\n",
    "    h = np.asarray(h, dtype=float)\n",
    "\n",
    "    if model_type == \"exponential\":\n",
    "        return nugget + partial_sill * (1.0 - np.exp(-h / max(rng_m, EPS)))\n",
    "\n",
    "    if model_type == \"gaussian\":\n",
    "        z = h / max(rng_m, EPS)\n",
    "        return nugget + partial_sill * (1.0 - np.exp(-(z * z)))\n",
    "\n",
    "    if model_type == \"spherical\":\n",
    "        r = h / max(rng_m, EPS)\n",
    "        out = np.empty_like(r, dtype=float)\n",
    "        inside = r < 1.0\n",
    "        out[inside] = nugget + partial_sill * (1.5 * r[inside] - 0.5 * r[inside] ** 3)\n",
    "        out[~inside] = nugget + partial_sill\n",
    "        return out\n",
    "\n",
    "    raise ValueError(f\"Unknown model_type: {model_type!r}\")\n",
    "\n",
    "\n",
    "def covariance_from_model(\n",
    "    model_type: str,\n",
    "    h: np.ndarray,\n",
    "    rng_m: float,\n",
    "    nugget: float,\n",
    "    partial_sill: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Covariance corresponding to the chosen model:\n",
    "    - off-diagonal: partial_sill * corr(h)\n",
    "    - diagonal: partial_sill + nugget\n",
    "    \"\"\"\n",
    "    h = np.asarray(h, dtype=float)\n",
    "\n",
    "    if model_type == \"exponential\":\n",
    "        c = partial_sill * np.exp(-h / max(rng_m, EPS))\n",
    "    elif model_type == \"gaussian\":\n",
    "        z = h / max(rng_m, EPS)\n",
    "        c = partial_sill * np.exp(-(z * z))\n",
    "    elif model_type == \"spherical\":\n",
    "        r = h / max(rng_m, EPS)\n",
    "        c = np.zeros_like(r, dtype=float)\n",
    "        inside = r < 1.0\n",
    "        c[inside] = partial_sill * (1.0 - 1.5 * r[inside] + 0.5 * r[inside] ** 3)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type!r}\")\n",
    "\n",
    "    return np.where(h == 0.0, partial_sill + nugget, c)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Empirical variogram (weighted; pair subsampling)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _sample_pairs(n: int, max_pairs: int, rng: np.random.Generator) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sample up to max_pairs index pairs (i < j) from n points.\"\"\"\n",
    "    if n < 2:\n",
    "        return np.array([], dtype=int), np.array([], dtype=int)\n",
    "\n",
    "    total_pairs = n * (n - 1) // 2\n",
    "    if total_pairs <= max_pairs:\n",
    "        i_idx, j_idx = np.triu_indices(n, k=1)\n",
    "        return i_idx.astype(int), j_idx.astype(int)\n",
    "\n",
    "    i_idx = rng.integers(0, n, size=max_pairs, dtype=np.int64)\n",
    "    j_idx = rng.integers(0, n, size=max_pairs, dtype=np.int64)\n",
    "    swap = i_idx > j_idx\n",
    "    i_idx[swap], j_idx[swap] = j_idx[swap], i_idx[swap]\n",
    "    neq = i_idx != j_idx\n",
    "    return i_idx[neq].astype(int), j_idx[neq].astype(int)\n",
    "\n",
    "\n",
    "def empirical_variogram_weighted(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    z: np.ndarray,\n",
    "    w: np.ndarray,\n",
    "    n_lags: int,\n",
    "    max_dist: float,\n",
    "    max_pairs: int,\n",
    "    seed: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Weighted empirical variogram:\n",
    "      gamma(h) = 0.5*(z_i - z_j)^2 aggregated in lag bins, weights w_i*w_j.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(z)\n",
    "    if n < 2:\n",
    "        return np.full(n_lags, np.nan), np.full(n_lags, np.nan), np.zeros(n_lags)\n",
    "\n",
    "    i_idx, j_idx = _sample_pairs(n, max_pairs=max_pairs, rng=rng)\n",
    "    if i_idx.size == 0:\n",
    "        return np.full(n_lags, np.nan), np.full(n_lags, np.nan), np.zeros(n_lags)\n",
    "\n",
    "    dx = x[i_idx] - x[j_idx]\n",
    "    dy = y[i_idx] - y[j_idx]\n",
    "    d = np.sqrt(dx * dx + dy * dy)\n",
    "\n",
    "    valid = d <= max_dist\n",
    "    if not np.any(valid):\n",
    "        return np.full(n_lags, np.nan), np.full(n_lags, np.nan), np.zeros(n_lags)\n",
    "\n",
    "    i_idx = i_idx[valid]\n",
    "    j_idx = j_idx[valid]\n",
    "    d = d[valid]\n",
    "\n",
    "    semivar = 0.5 * (z[i_idx] - z[j_idx]) ** 2\n",
    "    pw = (w[i_idx] * w[j_idx]).astype(float)\n",
    "\n",
    "    edges = np.linspace(0.0, max_dist, n_lags + 1)\n",
    "    bin_idx = np.searchsorted(edges, d, side=\"right\") - 1\n",
    "    bin_idx = np.clip(bin_idx, 0, n_lags - 1)\n",
    "\n",
    "    gamma_sum = np.zeros(n_lags, dtype=float)\n",
    "    weight_sum = np.zeros(n_lags, dtype=float)\n",
    "\n",
    "    np.add.at(gamma_sum, bin_idx, semivar * pw)\n",
    "    np.add.at(weight_sum, bin_idx, pw)\n",
    "\n",
    "    gamma_emp = np.where(weight_sum > 0, gamma_sum / weight_sum, np.nan)\n",
    "    lag_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    return lag_centers, gamma_emp, weight_sum\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Variogram fitting surface + best fit (grid search)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def fit_variogram_surface(\n",
    "    model_type: str,\n",
    "    lag: np.ndarray,\n",
    "    gamma_emp: np.ndarray,\n",
    "    bin_w: np.ndarray,\n",
    "    sill_total: float,\n",
    "    range_min: float,\n",
    "    range_max: float,\n",
    "    nugget_max: float,\n",
    "    range_grid_size: int,\n",
    "    nugget_grid_size: int,\n",
    ") -> Tuple[float, float, VariogramFitArtifacts]:\n",
    "    \"\"\"\n",
    "    Fit (range, nugget) for a fixed sill_total via grid search and return fit surface.\n",
    "    \"\"\"\n",
    "    ok = np.isfinite(gamma_emp) & (bin_w > 0)\n",
    "    if not np.any(ok):\n",
    "        rg = np.geomspace(max(range_min, EPS), max(range_max, range_min + EPS), range_grid_size)\n",
    "        ng = np.linspace(0.0, max(nugget_max, 0.0), nugget_grid_size)\n",
    "        sse = np.full((len(ng), len(rg)), np.nan, dtype=float)\n",
    "        best_r = float(rg[len(rg) // 2])\n",
    "        best_n = float(min(nugget_max, 0.05))\n",
    "        return best_r, best_n, VariogramFitArtifacts(lag, gamma_emp, bin_w, sill_total, rg, ng, sse)\n",
    "\n",
    "    lag2 = lag[ok]\n",
    "    emp2 = gamma_emp[ok]\n",
    "    w2 = bin_w[ok]\n",
    "\n",
    "    range_grid = np.geomspace(max(range_min, EPS), max(range_max, range_min + EPS), range_grid_size)\n",
    "    nugget_grid = np.linspace(0.0, max(nugget_max, 0.0), nugget_grid_size)\n",
    "\n",
    "    sse_grid = np.zeros((len(nugget_grid), len(range_grid)), dtype=float)\n",
    "    best_sse = np.inf\n",
    "    best_r = float(range_grid[len(range_grid) // 2])\n",
    "    best_n = float(min(nugget_max, 0.05))\n",
    "\n",
    "    for i, nug in enumerate(nugget_grid):\n",
    "        partial = max(sill_total - nug, 0.0)\n",
    "        for j, rr in enumerate(range_grid):\n",
    "            mod = variogram_gamma(model_type, lag2, rr, nug, partial)\n",
    "            sse = float(np.sum(w2 * (emp2 - mod) ** 2))\n",
    "            sse_grid[i, j] = sse\n",
    "            if sse < best_sse:\n",
    "                best_sse = sse\n",
    "                best_r = float(rr)\n",
    "                best_n = float(nug)\n",
    "\n",
    "    art = VariogramFitArtifacts(\n",
    "        lag=lag,\n",
    "        gamma_emp=gamma_emp,\n",
    "        bin_w=bin_w,\n",
    "        sill_total=sill_total,\n",
    "        range_grid=range_grid,\n",
    "        nugget_grid=nugget_grid,\n",
    "        sse_grid=sse_grid,\n",
    "    )\n",
    "    return best_r, best_n, art\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Local OK prediction for binary J (CV scoring)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class NeighborIndex:\n",
    "    \"\"\"Radius neighbor search using scipy.cKDTree or sklearn NearestNeighbors.\"\"\"\n",
    "\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.xy = np.column_stack([x, y]).astype(float)\n",
    "        self._backend = None\n",
    "        self._tree = None\n",
    "\n",
    "        try:\n",
    "            from scipy.spatial import cKDTree  # type: ignore\n",
    "\n",
    "            self._backend = \"scipy\"\n",
    "            self._tree = cKDTree(self.xy)\n",
    "        except Exception:\n",
    "            try:\n",
    "                from sklearn.neighbors import NearestNeighbors  # type: ignore\n",
    "\n",
    "                self._backend = \"sklearn\"\n",
    "                self._tree = NearestNeighbors(algorithm=\"ball_tree\").fit(self.xy)\n",
    "            except Exception as exc:\n",
    "                raise ImportError(\"Need scipy or scikit-learn for neighbor search.\") from exc\n",
    "\n",
    "    def query_neighbors(self, targets: np.ndarray, radius: float, max_neighbors: int) -> List[np.ndarray]:\n",
    "        if self._backend == \"scipy\":\n",
    "            idx_list = self._tree.query_ball_point(targets, r=radius)  # type: ignore\n",
    "            out: List[np.ndarray] = []\n",
    "            for i, idx in enumerate(idx_list):\n",
    "                if not idx:\n",
    "                    out.append(np.array([], dtype=int))\n",
    "                    continue\n",
    "                pts = self.xy[np.array(idx, dtype=int)]\n",
    "                d = np.sqrt(np.sum((pts - targets[i]) ** 2, axis=1))\n",
    "                order = np.argsort(d)[:max_neighbors]\n",
    "                out.append(np.array(idx, dtype=int)[order])\n",
    "            return out\n",
    "\n",
    "        n = self.xy.shape[0]\n",
    "        k = min(n, max_neighbors)\n",
    "        d, idx = self._tree.kneighbors(targets, n_neighbors=k, return_distance=True)  # type: ignore\n",
    "        out = []\n",
    "        for di, ii in zip(d, idx):\n",
    "            out.append(ii[di <= radius])\n",
    "        return out\n",
    "\n",
    "\n",
    "def ok_predict_binary_local(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    train_z: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    model: VariogramModel,\n",
    "    search_radius_m: float,\n",
    "    min_neighbors: int,\n",
    "    max_neighbors: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Local ordinary kriging prediction for binary variable train_z at target locations.\"\"\"\n",
    "    nugget = float(model.nugget)\n",
    "    sill_total = float(model.sill_total)\n",
    "    partial = max(sill_total - nugget, 0.0)\n",
    "\n",
    "    idx = NeighborIndex(train_x, train_y)\n",
    "    neigh_list = idx.query_neighbors(targets, radius=search_radius_m, max_neighbors=max_neighbors)\n",
    "\n",
    "    p = np.full(targets.shape[0], np.nan, dtype=float)\n",
    "    xy_train = np.column_stack([train_x, train_y]).astype(float)\n",
    "\n",
    "    for i, nn in enumerate(neigh_list):\n",
    "        if nn.size < min_neighbors:\n",
    "            p[i] = float(np.mean(train_z))\n",
    "            continue\n",
    "\n",
    "        pts = xy_train[nn]\n",
    "        z = train_z[nn].astype(float)\n",
    "\n",
    "        dx = pts[:, 0][:, None] - pts[:, 0][None, :]\n",
    "        dy = pts[:, 1][:, None] - pts[:, 1][None, :]\n",
    "        h = np.sqrt(dx * dx + dy * dy)\n",
    "        C = covariance_from_model(model.model_type, h, model.range_m, nugget, partial)\n",
    "\n",
    "        m = len(nn)\n",
    "        A = np.zeros((m + 1, m + 1), dtype=float)\n",
    "        A[:m, :m] = C\n",
    "        A[:m, m] = 1.0\n",
    "        A[m, :m] = 1.0\n",
    "\n",
    "        d0 = np.sqrt((pts[:, 0] - targets[i, 0]) ** 2 + (pts[:, 1] - targets[i, 1]) ** 2)\n",
    "        c0 = covariance_from_model(model.model_type, d0, model.range_m, nugget, partial)\n",
    "\n",
    "        b = np.zeros(m + 1, dtype=float)\n",
    "        b[:m] = c0\n",
    "        b[m] = 1.0\n",
    "\n",
    "        try:\n",
    "            sol = np.linalg.solve(A, b)\n",
    "            w = sol[:m]\n",
    "        except np.linalg.LinAlgError:\n",
    "            jitter = 1e-8 * (np.trace(C) / max(m, 1))\n",
    "            A[:m, :m] += np.eye(m) * jitter\n",
    "            try:\n",
    "                sol = np.linalg.solve(A, b)\n",
    "                w = sol[:m]\n",
    "            except np.linalg.LinAlgError:\n",
    "                p[i] = float(np.mean(train_z))\n",
    "                continue\n",
    "\n",
    "        p[i] = float(np.clip(np.dot(w, z), 0.0, 1.0))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def log_loss_binary(y_true: np.ndarray, p_hat: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    \"\"\"Binary log loss.\"\"\"\n",
    "    p = np.clip(p_hat, eps, 1.0 - eps)\n",
    "    y = y_true.astype(float)\n",
    "    return float(np.mean(-(y * np.log(p) + (1.0 - y) * np.log(1.0 - p))))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Diagnostics plotting (PNG)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def plot_variogram_fit_png(\n",
    "    out_png: Path,\n",
    "    lag: np.ndarray,\n",
    "    gamma_emp: np.ndarray,\n",
    "    bin_w: np.ndarray,\n",
    "    fits: Dict[str, Tuple[float, float, float]],\n",
    "    max_dist: float,\n",
    "    title: str,\n",
    ") -> None:\n",
    "    \"\"\"Plot empirical variogram + fitted curves for exp/sph/gau.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "    ok = np.isfinite(gamma_emp) & (bin_w > 0)\n",
    "    ax.scatter(lag[ok], gamma_emp[ok], s=35, label=\"Empirical (weighted bins)\")\n",
    "\n",
    "    xs = np.linspace(0.0, max_dist, 300)\n",
    "    for mt, (rr, nug, sill_total) in fits.items():\n",
    "        partial = max(sill_total - nug, 0.0)\n",
    "        ys = variogram_gamma(mt, xs, rr, nug, partial)\n",
    "        ax.plot(xs, ys, label=f\"{mt}: range={rr:.1f}, nug={nug:.3f}\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Lag distance (m)\")\n",
    "    ax.set_ylabel(\"Semivariance γ(h)\")\n",
    "    ax.set_xlim(0, max_dist)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc=\"best\", frameon=True)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_cv_scores_png(out_png: Path, cv_df: pd.DataFrame, title: str) -> None:\n",
    "    \"\"\"Boxplot of per-fold log loss by model.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    models = [\"exponential\", \"spherical\", \"gaussian\"]\n",
    "    data = [cv_df.loc[cv_df[\"model_type\"] == m, \"log_loss\"].values for m in models]\n",
    "    ax.boxplot(data, tick_labels=models, showmeans=True)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Log loss (lower is better)\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_variogram_surface_png(out_png: Path, art: VariogramFitArtifacts, best_r: float, best_n: float, title: str) -> None:\n",
    "    \"\"\"Heatmap of SSE objective over (range, nugget).\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    sse_plot = np.log10(np.maximum(art.sse_grid, EPS))\n",
    "    im = ax.imshow(\n",
    "        sse_plot,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        interpolation=\"nearest\",\n",
    "        extent=[\n",
    "            float(art.range_grid[0]),\n",
    "            float(art.range_grid[-1]),\n",
    "            float(art.nugget_grid[0]),\n",
    "            float(art.nugget_grid[-1]),\n",
    "        ],\n",
    "    )\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"log10(weighted SSE)\")\n",
    "\n",
    "    ax.scatter([best_r], [best_n], marker=\"x\", s=120)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Range (m) [log scale]\")\n",
    "    ax.set_ylabel(\"Nugget\")\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_realizations_panel_png(out_png: Path, realizations: List[np.ndarray], inside: np.ndarray, title: str) -> None:\n",
    "    \"\"\"Plot a panel of categorical realizations (uint8 codes).\"\"\"\n",
    "    n = len(realizations)\n",
    "    if n == 0:\n",
    "        return\n",
    "\n",
    "    ncols = int(math.ceil(math.sqrt(n)))\n",
    "    nrows = int(math.ceil(n / ncols))\n",
    "\n",
    "    cmap = ListedColormap(PALETTE)\n",
    "    norm = BoundaryNorm(boundaries=np.arange(-0.5, 6.5, 1.0), ncolors=cmap.N)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.axis(\"off\")\n",
    "        if i >= n:\n",
    "            continue\n",
    "        arr = realizations[i].copy()\n",
    "        arr[~inside] = 0\n",
    "        ax.imshow(arr, cmap=cmap, norm=norm, interpolation=\"nearest\")\n",
    "        ax.set_title(f\"Panel sample {i + 1}\")\n",
    "\n",
    "    legend_handles = [Patch(facecolor=PALETTE[c], edgecolor=\"none\", label=CODE_TO_CLASS[c]) for c in range(1, 6)]\n",
    "    legend_handles.insert(0, Patch(facecolor=\"#ffffff\", edgecolor=\"k\", label=\"Uncertain/Masked\"))\n",
    "\n",
    "    fig.suptitle(title, y=0.995)\n",
    "    fig.legend(handles=legend_handles, loc=\"lower center\", ncol=3, frameon=True)\n",
    "    fig.tight_layout(rect=[0, 0.06, 1, 0.96])\n",
    "\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CV selection + per-unit fits for diagnostics\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def select_shared_variogram_by_block_cv(unit_data: UnitData, seed: int) -> Tuple[VariogramModel, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Select variogram family + (range, nugget) for binary J via spatial block CV.\n",
    "\n",
    "    Returns:\n",
    "      best_model_fitted_on_all_data, cv_scores_df\n",
    "    \"\"\"\n",
    "    dx = float(np.max(unit_data.x) - np.min(unit_data.x))\n",
    "    dy = float(np.max(unit_data.y) - np.min(unit_data.y))\n",
    "    span = float(max(math.hypot(dx, dy), 1.0))\n",
    "\n",
    "    nn_med = estimate_median_nn_distance(unit_data.x, unit_data.y)\n",
    "    decluster_cell = float(np.clip(DECLUSTER_MULT * nn_med, DECLUSTER_CELL_MIN_M, DECLUSTER_CELL_MAX_M))\n",
    "    block_size = float(np.clip(BLOCK_SIZE_MULT * nn_med, BLOCK_SIZE_MIN_M, BLOCK_SIZE_MAX_M))\n",
    "\n",
    "    block_ids = assign_blocks(unit_data.x, unit_data.y, block_size_m=block_size)\n",
    "    folds = make_block_folds(block_ids, n_folds=CV_FOLDS, seed=seed)\n",
    "\n",
    "    model_types = [\"exponential\", \"spherical\", \"gaussian\"]\n",
    "    rows: List[Dict[str, object]] = []\n",
    "\n",
    "    max_dist = float(max(MAX_DIST_FRACTION_OF_SPAN * span, 3.0 * GRID_CELL_SIZE_M))\n",
    "    range_min = float(max(3.0 * GRID_CELL_SIZE_M, 10.0))\n",
    "    range_max = float(max(min(0.5 * span, 250.0), range_min + EPS))\n",
    "\n",
    "    for fold_i, test_mask in enumerate(folds, start=1):\n",
    "        train_mask = ~test_mask\n",
    "        if np.sum(train_mask) < max(MIN_NEIGHBORS + 2, 20) or np.sum(test_mask) < 5:\n",
    "            continue\n",
    "\n",
    "        w_train = compute_declustering_weights(\n",
    "            unit_data.x[train_mask],\n",
    "            unit_data.y[train_mask],\n",
    "            cell_size_m=decluster_cell,\n",
    "        )\n",
    "\n",
    "        p_train = weighted_prevalence(unit_data.code[train_mask], w_train)\n",
    "        dominant_code = int(np.argmax(p_train) + 1)\n",
    "\n",
    "        j_train = (unit_data.code[train_mask] != dominant_code).astype(float)\n",
    "        j_test = (unit_data.code[test_mask] != dominant_code).astype(float)\n",
    "\n",
    "        p_j = float(np.sum(w_train * j_train) / max(np.sum(w_train), EPS))\n",
    "        sill_total_j = float(max(p_j * (1.0 - p_j), EPS))\n",
    "\n",
    "        lag, gamma_emp, bin_w = empirical_variogram_weighted(\n",
    "            x=unit_data.x[train_mask],\n",
    "            y=unit_data.y[train_mask],\n",
    "            z=j_train,\n",
    "            w=w_train,\n",
    "            n_lags=N_LAGS,\n",
    "            max_dist=max_dist,\n",
    "            max_pairs=MAX_PAIRS_FOR_VARIOGRAM,\n",
    "            seed=seed + 1000 * fold_i,\n",
    "        )\n",
    "\n",
    "        for mt in model_types:\n",
    "            best_r, best_n, _ = fit_variogram_surface(\n",
    "                model_type=mt,\n",
    "                lag=lag,\n",
    "                gamma_emp=gamma_emp,\n",
    "                bin_w=bin_w,\n",
    "                sill_total=sill_total_j,\n",
    "                range_min=range_min,\n",
    "                range_max=range_max,\n",
    "                nugget_max=min(NUGGET_MAX, sill_total_j),\n",
    "                range_grid_size=RANGE_GRID_SIZE,\n",
    "                nugget_grid_size=NUGGET_GRID_SIZE,\n",
    "            )\n",
    "\n",
    "            model_fold = VariogramModel(mt, best_r, best_n, sill_total_j)\n",
    "            search_radius = float(SEARCH_RADIUS_FACTOR * best_r)\n",
    "\n",
    "            p_hat = ok_predict_binary_local(\n",
    "                train_x=unit_data.x[train_mask],\n",
    "                train_y=unit_data.y[train_mask],\n",
    "                train_z=j_train,\n",
    "                targets=np.column_stack([unit_data.x[test_mask], unit_data.y[test_mask]]).astype(float),\n",
    "                model=model_fold,\n",
    "                search_radius_m=search_radius,\n",
    "                min_neighbors=MIN_NEIGHBORS,\n",
    "                max_neighbors=MAX_NEIGHBORS,\n",
    "            )\n",
    "\n",
    "            score = log_loss_binary(j_test, p_hat, eps=1e-6)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"fold\": fold_i,\n",
    "                    \"model_type\": mt,\n",
    "                    \"log_loss\": float(score),\n",
    "                    \"range_m\": float(best_r),\n",
    "                    \"nugget\": float(best_n),\n",
    "                    \"sill_total_J\": float(sill_total_j),\n",
    "                    \"dominant_code\": int(dominant_code),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    cv_df = pd.DataFrame(rows)\n",
    "    if cv_df.empty:\n",
    "        # fallback: heuristic exponential\n",
    "        w_all = compute_declustering_weights(unit_data.x, unit_data.y, cell_size_m=decluster_cell)\n",
    "        p_all = weighted_prevalence(unit_data.code, w_all)\n",
    "        dominant_code = int(np.argmax(p_all) + 1)\n",
    "        j_all = (unit_data.code != dominant_code).astype(float)\n",
    "        p_j = float(np.sum(w_all * j_all) / max(np.sum(w_all), EPS))\n",
    "        sill_total = float(max(p_j * (1.0 - p_j), EPS))\n",
    "        rng_m = float(np.clip(8.0 * nn_med, 10.0, 200.0))\n",
    "        nug = float(min(0.05, sill_total))\n",
    "        return VariogramModel(\"exponential\", rng_m, nug, sill_total), cv_df\n",
    "\n",
    "    mean_scores = cv_df.groupby(\"model_type\")[\"log_loss\"].mean().to_dict()\n",
    "    best_family = min(mean_scores, key=mean_scores.get)\n",
    "\n",
    "    # Final fit on ALL data for chosen family\n",
    "    w_all = compute_declustering_weights(unit_data.x, unit_data.y, cell_size_m=decluster_cell)\n",
    "    p_all = weighted_prevalence(unit_data.code, w_all)\n",
    "    dominant_code = int(np.argmax(p_all) + 1)\n",
    "    j_all = (unit_data.code != dominant_code).astype(float)\n",
    "\n",
    "    p_j = float(np.sum(w_all * j_all) / max(np.sum(w_all), EPS))\n",
    "    sill_total = float(max(p_j * (1.0 - p_j), EPS))\n",
    "\n",
    "    lag, gamma_emp, bin_w = empirical_variogram_weighted(\n",
    "        x=unit_data.x,\n",
    "        y=unit_data.y,\n",
    "        z=j_all,\n",
    "        w=w_all,\n",
    "        n_lags=N_LAGS,\n",
    "        max_dist=max_dist,\n",
    "        max_pairs=MAX_PAIRS_FOR_VARIOGRAM,\n",
    "        seed=seed + 9999,\n",
    "    )\n",
    "\n",
    "    best_r, best_n, _ = fit_variogram_surface(\n",
    "        model_type=best_family,\n",
    "        lag=lag,\n",
    "        gamma_emp=gamma_emp,\n",
    "        bin_w=bin_w,\n",
    "        sill_total=sill_total,\n",
    "        range_min=range_min,\n",
    "        range_max=range_max,\n",
    "        nugget_max=min(NUGGET_MAX, sill_total),\n",
    "        range_grid_size=RANGE_GRID_SIZE,\n",
    "        nugget_grid_size=NUGGET_GRID_SIZE,\n",
    "    )\n",
    "\n",
    "    return VariogramModel(best_family, best_r, best_n, sill_total), cv_df\n",
    "\n",
    "\n",
    "def build_unit_J_all(unit_data: UnitData, seed: int) -> Tuple[np.ndarray, np.ndarray, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Build binary J on ALL data and return:\n",
    "      j_all, w_all, sill_total_J, max_dist, range_min, range_max\n",
    "    \"\"\"\n",
    "    dx = float(np.max(unit_data.x) - np.min(unit_data.x))\n",
    "    dy = float(np.max(unit_data.y) - np.min(unit_data.y))\n",
    "    span = float(max(math.hypot(dx, dy), 1.0))\n",
    "\n",
    "    nn_med = estimate_median_nn_distance(unit_data.x, unit_data.y)\n",
    "    decluster_cell = float(np.clip(DECLUSTER_MULT * nn_med, DECLUSTER_CELL_MIN_M, DECLUSTER_CELL_MAX_M))\n",
    "    w_all = compute_declustering_weights(unit_data.x, unit_data.y, cell_size_m=decluster_cell)\n",
    "\n",
    "    p_all = weighted_prevalence(unit_data.code, w_all)\n",
    "    dominant_code = int(np.argmax(p_all) + 1)\n",
    "    j_all = (unit_data.code != dominant_code).astype(float)\n",
    "\n",
    "    p_j = float(np.sum(w_all * j_all) / max(np.sum(w_all), EPS))\n",
    "    sill_total = float(max(p_j * (1.0 - p_j), EPS))\n",
    "\n",
    "    max_dist = float(max(MAX_DIST_FRACTION_OF_SPAN * span, 3.0 * GRID_CELL_SIZE_M))\n",
    "    range_min = float(max(3.0 * GRID_CELL_SIZE_M, 10.0))\n",
    "    range_max = float(max(min(0.5 * span, 250.0), range_min + EPS))\n",
    "\n",
    "    return j_all, w_all, sill_total, max_dist, range_min, range_max\n",
    "\n",
    "\n",
    "def write_unit_variogram_diagnostics(\n",
    "    unit_data: UnitData,\n",
    "    unit_out: Path,\n",
    "    best_model: VariogramModel,\n",
    "    cv_df: pd.DataFrame,\n",
    "    seed: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Diagnostics bundle items:\n",
    "    1) variogram_J_fit.png\n",
    "    2) cv_scores.csv + cv_scores.png\n",
    "    4) variogram_fit_surface_<best_model>.png\n",
    "    \"\"\"\n",
    "    unit_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # (2) CV scores\n",
    "    (unit_out / \"cv_scores.csv\").write_text(cv_df.to_csv(index=False))\n",
    "    if not cv_df.empty:\n",
    "        plot_cv_scores_png(\n",
    "            out_png=unit_out / \"cv_scores.png\",\n",
    "            cv_df=cv_df,\n",
    "            title=f\"{unit_data.unit} - Block CV log loss by variogram family\",\n",
    "        )\n",
    "\n",
    "    # Empirical variogram for J on ALL data\n",
    "    j_all, w_all, sill_total, max_dist, range_min, range_max = build_unit_J_all(unit_data, seed=seed)\n",
    "    lag, gamma_emp, bin_w = empirical_variogram_weighted(\n",
    "        x=unit_data.x,\n",
    "        y=unit_data.y,\n",
    "        z=j_all,\n",
    "        w=w_all,\n",
    "        n_lags=N_LAGS,\n",
    "        max_dist=max_dist,\n",
    "        max_pairs=MAX_PAIRS_FOR_VARIOGRAM,\n",
    "        seed=seed + 4242,\n",
    "    )\n",
    "\n",
    "    # Fit each family on ALL data for overlay curves (diagnostic #1)\n",
    "    fits: Dict[str, Tuple[float, float, float]] = {}\n",
    "    surfaces: Dict[str, VariogramFitArtifacts] = {}\n",
    "    params: Dict[str, Tuple[float, float]] = {}\n",
    "\n",
    "    for mt in [\"exponential\", \"spherical\", \"gaussian\"]:\n",
    "        rr, nug, art = fit_variogram_surface(\n",
    "            model_type=mt,\n",
    "            lag=lag,\n",
    "            gamma_emp=gamma_emp,\n",
    "            bin_w=bin_w,\n",
    "            sill_total=sill_total,\n",
    "            range_min=range_min,\n",
    "            range_max=range_max,\n",
    "            nugget_max=min(NUGGET_MAX, sill_total),\n",
    "            range_grid_size=RANGE_GRID_SIZE,\n",
    "            nugget_grid_size=NUGGET_GRID_SIZE,\n",
    "        )\n",
    "        fits[mt] = (float(rr), float(nug), float(sill_total))\n",
    "        surfaces[mt] = art\n",
    "        params[mt] = (float(rr), float(nug))\n",
    "\n",
    "    plot_variogram_fit_png(\n",
    "        out_png=unit_out / \"variogram_J_fit.png\",\n",
    "        lag=lag,\n",
    "        gamma_emp=gamma_emp,\n",
    "        bin_w=bin_w,\n",
    "        fits=fits,\n",
    "        max_dist=max_dist,\n",
    "        title=f\"{unit_data.unit} - Empirical variogram for J (dominant vs rest) with fitted curves\",\n",
    "    )\n",
    "\n",
    "    # (4) Parameter surface for selected family\n",
    "    best_mt = best_model.model_type\n",
    "    best_r, best_n = params[best_mt]\n",
    "    art_best = surfaces[best_mt]\n",
    "    plot_variogram_surface_png(\n",
    "        out_png=unit_out / f\"variogram_fit_surface_{best_mt}.png\",\n",
    "        art=art_best,\n",
    "        best_r=best_r,\n",
    "        best_n=best_n,\n",
    "        title=f\"{unit_data.unit} - Variogram fit surface ({best_mt})\",\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Class-specific sills and shrinkage\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def compute_class_sills(unit_data: UnitData) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute marginal prevalence p_k and total sills sill_k = p_k(1-p_k) with declustering.\n",
    "    \"\"\"\n",
    "    nn_med = estimate_median_nn_distance(unit_data.x, unit_data.y)\n",
    "    decluster_cell = float(np.clip(DECLUSTER_MULT * nn_med, DECLUSTER_CELL_MIN_M, DECLUSTER_CELL_MAX_M))\n",
    "    w = compute_declustering_weights(unit_data.x, unit_data.y, cell_size_m=decluster_cell)\n",
    "    p = weighted_prevalence(unit_data.code, w)\n",
    "\n",
    "    total = p * (1.0 - p)\n",
    "    if SILL_FLOOR and float(SILL_FLOOR) > 0:\n",
    "        total = np.maximum(total, float(SILL_FLOOR))\n",
    "\n",
    "    return p, total\n",
    "\n",
    "\n",
    "def total_to_partial_sills(total_sills: np.ndarray, nugget_shared: float) -> np.ndarray:\n",
    "    \"\"\"partial_sill_k = max(total_sill_k - nugget, 0).\"\"\"\n",
    "    return np.maximum(total_sills - float(nugget_shared), 0.0)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SIS conditioning set (spatial hash)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class SpatialHash:\n",
    "    \"\"\"Simple spatial hash for incremental neighborhood queries in SIS.\"\"\"\n",
    "\n",
    "    def __init__(self, xmin: float, ymin: float, cell_size_m: float):\n",
    "        self.xmin = float(xmin)\n",
    "        self.ymin = float(ymin)\n",
    "        self.cell = float(cell_size_m)\n",
    "        self._cells: Dict[Tuple[int, int], List[CondPoint]] = {}\n",
    "\n",
    "    def _cell_index(self, x: float, y: float) -> Tuple[int, int]:\n",
    "        cx = int(math.floor((x - self.xmin) / self.cell))\n",
    "        cy = int(math.floor((y - self.ymin) / self.cell))\n",
    "        return cx, cy\n",
    "\n",
    "    def add_point(self, p: CondPoint) -> None:\n",
    "        idx = self._cell_index(p.x, p.y)\n",
    "        self._cells.setdefault(idx, []).append(p)\n",
    "\n",
    "    def add_many(self, x: np.ndarray, y: np.ndarray, code: np.ndarray) -> None:\n",
    "        for xi, yi, ci in zip(x, y, code):\n",
    "            self.add_point(CondPoint(float(xi), float(yi), int(ci)))\n",
    "\n",
    "    def query_radius(self, x0: float, y0: float, radius_m: float, max_neighbors: int) -> List[CondPoint]:\n",
    "        r = float(radius_m)\n",
    "        cx, cy = self._cell_index(x0, y0)\n",
    "        n = int(math.ceil(r / max(self.cell, EPS)))\n",
    "\n",
    "        candidates: List[CondPoint] = []\n",
    "        for dx in range(-n, n + 1):\n",
    "            for dy in range(-n, n + 1):\n",
    "                cell = (cx + dx, cy + dy)\n",
    "                if cell in self._cells:\n",
    "                    candidates.extend(self._cells[cell])\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        pts = np.array([(p.x, p.y) for p in candidates], dtype=float)\n",
    "        d = np.sqrt((pts[:, 0] - x0) ** 2 + (pts[:, 1] - y0) ** 2)\n",
    "        m = d <= r\n",
    "        if not np.any(m):\n",
    "            return []\n",
    "\n",
    "        d = d[m]\n",
    "        candidates = [candidates[i] for i in np.where(m)[0]]\n",
    "        order = np.argsort(d)[:max_neighbors]\n",
    "        return [candidates[i] for i in order]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SIS probability mechanics (OK weights + shrinkage)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def ok_weights(\n",
    "    cond_pts: List[CondPoint],\n",
    "    x0: float,\n",
    "    y0: float,\n",
    "    model_type: str,\n",
    "    range_m: float,\n",
    "    nugget: float,\n",
    "    partial_sill_ref: float,\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Ordinary kriging weights for a target location using a reference sill scale.\n",
    "    \"\"\"\n",
    "    m = len(cond_pts)\n",
    "    if m == 0:\n",
    "        return None\n",
    "\n",
    "    xy = np.array([(p.x, p.y) for p in cond_pts], dtype=float)\n",
    "    dx = xy[:, 0][:, None] - xy[:, 0][None, :]\n",
    "    dy = xy[:, 1][:, None] - xy[:, 1][None, :]\n",
    "    h = np.sqrt(dx * dx + dy * dy)\n",
    "    C = covariance_from_model(model_type, h, range_m, nugget, partial_sill_ref)\n",
    "\n",
    "    A = np.zeros((m + 1, m + 1), dtype=float)\n",
    "    A[:m, :m] = C\n",
    "    A[:m, m] = 1.0\n",
    "    A[m, :m] = 1.0\n",
    "\n",
    "    d0 = np.sqrt((xy[:, 0] - x0) ** 2 + (xy[:, 1] - y0) ** 2)\n",
    "    c0 = covariance_from_model(model_type, d0, range_m, nugget, partial_sill_ref)\n",
    "\n",
    "    b = np.zeros(m + 1, dtype=float)\n",
    "    b[:m] = c0\n",
    "    b[m] = 1.0\n",
    "\n",
    "    try:\n",
    "        sol = np.linalg.solve(A, b)\n",
    "        return sol[:m]\n",
    "    except np.linalg.LinAlgError:\n",
    "        jitter = 1e-8 * (np.trace(C) / max(m, 1))\n",
    "        A[:m, :m] += np.eye(m) * jitter\n",
    "        try:\n",
    "            sol = np.linalg.solve(A, b)\n",
    "            return sol[:m]\n",
    "        except np.linalg.LinAlgError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def conditional_probs_sis(\n",
    "    cond_pts: List[CondPoint],\n",
    "    weights: Optional[np.ndarray],\n",
    "    present_codes: List[int],\n",
    "    marginal_p: np.ndarray,\n",
    "    partial_sills: np.ndarray,\n",
    "    nugget: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert OK indicator means into conditional probabilities using sill-based shrinkage.\n",
    "    \"\"\"\n",
    "    probs = np.zeros(5, dtype=float)\n",
    "\n",
    "    if weights is None or len(cond_pts) == 0:\n",
    "        return marginal_p.copy()\n",
    "\n",
    "    codes = np.array([p.code for p in cond_pts], dtype=int)\n",
    "    w = weights.astype(float)\n",
    "\n",
    "    for code in present_codes:\n",
    "        ind = (codes == code).astype(float)\n",
    "        m_k = float(np.dot(w, ind))\n",
    "        m_k = float(np.clip(m_k, 0.0, 1.0))\n",
    "\n",
    "        ps = float(partial_sills[code - 1])\n",
    "        lam = ps / max(ps + float(nugget), EPS)\n",
    "        probs[code - 1] = lam * m_k + (1.0 - lam) * float(marginal_p[code - 1])\n",
    "\n",
    "    probs = np.clip(probs, 0.0, 1.0)\n",
    "    s = float(np.sum(probs))\n",
    "    if s <= 0.0 or not np.isfinite(s):\n",
    "        return marginal_p.copy()\n",
    "\n",
    "    return probs / s\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Point-to-grid imprint + MMU cleanup\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def rasterize_points_to_grid(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    code: np.ndarray,\n",
    "    xmin: float,\n",
    "    ymax: float,\n",
    "    cell_size: float,\n",
    "    height: int,\n",
    "    width: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Imprint observed points onto the grid at cell resolution.\"\"\"\n",
    "    col = np.floor((x - xmin) / cell_size).astype(int)\n",
    "    row = np.floor((ymax - y) / cell_size).astype(int)\n",
    "\n",
    "    fixed = np.zeros((height, width), dtype=np.uint8)\n",
    "    ok = (row >= 0) & (row < height) & (col >= 0) & (col < width)\n",
    "    fixed[row[ok], col[ok]] = code[ok].astype(np.uint8)\n",
    "    return fixed\n",
    "\n",
    "\n",
    "def fill_uncertain_by_majority(cat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Fill zeros using 3x3 neighborhood majority class (excluding zero).\"\"\"\n",
    "    try:\n",
    "        from scipy import ndimage  # type: ignore\n",
    "    except Exception:\n",
    "        return cat\n",
    "\n",
    "    out = cat.copy()\n",
    "    uncertain = out == 0\n",
    "    if not np.any(uncertain):\n",
    "        return out\n",
    "\n",
    "    kernel = np.ones((3, 3), dtype=int)\n",
    "    class_counts = []\n",
    "    for c in range(1, 6):\n",
    "        class_counts.append(ndimage.convolve((out == c).astype(int), kernel, mode=\"nearest\"))\n",
    "\n",
    "    counts = np.stack(class_counts, axis=0)\n",
    "    winner = np.argmax(counts, axis=0) + 1\n",
    "    max_count = np.max(counts, axis=0)\n",
    "\n",
    "    fillable = uncertain & (max_count > 0)\n",
    "    out[fillable] = winner[fillable].astype(out.dtype)\n",
    "    return out\n",
    "\n",
    "\n",
    "def mmu_cleanup(cat: np.ndarray, mmu_cells: int, n_iters: int = 5) -> np.ndarray:\n",
    "    \"\"\"Remove patches smaller than mmu_cells and fill by neighborhood majority.\"\"\"\n",
    "    try:\n",
    "        from scipy import ndimage  # type: ignore\n",
    "    except Exception:\n",
    "        print(\"WARNING: scipy not available; skipping MMU cleanup.\")\n",
    "        return cat\n",
    "\n",
    "    out = cat.copy()\n",
    "    structure = np.ones((3, 3), dtype=int)\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        changed = False\n",
    "        for c in range(1, 6):\n",
    "            mask = out == c\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            labeled, nlab = ndimage.label(mask, structure=structure)\n",
    "            if nlab == 0:\n",
    "                continue\n",
    "\n",
    "            sizes = np.bincount(labeled.ravel())\n",
    "            small = np.where((sizes < mmu_cells) & (np.arange(len(sizes)) != 0))[0]\n",
    "            if len(small) == 0:\n",
    "                continue\n",
    "\n",
    "            small_mask = np.isin(labeled, small)\n",
    "            if np.any(small_mask):\n",
    "                out[small_mask] = 0\n",
    "                changed = True\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "        out = fill_uncertain_by_majority(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# GeoTIFF writing\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def write_geotiff(\n",
    "    path: Path,\n",
    "    array: np.ndarray,\n",
    "    transform: rasterio.Affine,\n",
    "    crs_epsg: int,\n",
    "    nodata: Optional[float],\n",
    "    dtype: str,\n",
    ") -> None:\n",
    "    \"\"\"Write a single-band GeoTIFF.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    height, width = array.shape\n",
    "    profile = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": dtype,\n",
    "        \"crs\": rasterio.crs.CRS.from_epsg(crs_epsg),\n",
    "        \"transform\": transform,\n",
    "        \"compress\": \"deflate\",\n",
    "        \"predictor\": 2 if np.issubdtype(array.dtype, np.floating) else 1,\n",
    "        \"tiled\": True,\n",
    "        \"blockxsize\": 256,\n",
    "        \"blockysize\": 256,\n",
    "    }\n",
    "    if nodata is not None:\n",
    "        profile[\"nodata\"] = nodata\n",
    "\n",
    "    with rasterio.open(path, \"w\", **profile) as dst:\n",
    "        dst.write(array, 1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SIS per unit (includes SAFE p_max / winner computation)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def run_sis_for_unit(unit_data: UnitData, out_dir: Path, variogram: VariogramModel, seed: int) -> Dict[str, object]:\n",
    "    unit_name = safe_name(unit_data.unit)\n",
    "    unit_out = out_dir / unit_name\n",
    "    unit_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = unit_bbox(unit_data, buffer_m=EXTENT_BUFFER_M)\n",
    "    x_centers, y_centers, transform = build_grid(xmin, ymin, xmax, ymax, cell_size=GRID_CELL_SIZE_M)\n",
    "    width = len(x_centers)\n",
    "    height = len(y_centers)\n",
    "\n",
    "    xx, yy = np.meshgrid(x_centers, y_centers)\n",
    "    grid_x = xx.astype(float)\n",
    "    grid_y = yy.astype(float)\n",
    "\n",
    "    if USE_POINT_ENVELOPE_MASK:\n",
    "        geoms = build_point_envelope_geometry(unit_data.x, unit_data.y, buffer_m=ENVELOPE_BUFFER_M)\n",
    "        inside = envelope_mask_for_grid(geoms, out_shape=(height, width), transform=transform)\n",
    "    else:\n",
    "        inside = np.ones((height, width), dtype=bool)\n",
    "\n",
    "    inside_lin = np.flatnonzero(inside.ravel())\n",
    "    if inside_lin.size == 0:\n",
    "        return {\"strManagementUnit\": unit_data.unit, \"status\": \"skipped_empty_envelope\"}\n",
    "\n",
    "    marginal_p, total_sills = compute_class_sills(unit_data)\n",
    "    partial_sills = total_to_partial_sills(total_sills, nugget_shared=variogram.nugget)\n",
    "\n",
    "    partial_ref = max(float(variogram.sill_total - variogram.nugget), float(np.nanmedian(partial_sills)), EPS)\n",
    "    search_radius = float(SEARCH_RADIUS_FACTOR * variogram.range_m)\n",
    "\n",
    "    fixed_code = rasterize_points_to_grid(\n",
    "        x=unit_data.x,\n",
    "        y=unit_data.y,\n",
    "        code=unit_data.code,\n",
    "        xmin=xmin,\n",
    "        ymax=ymax,\n",
    "        cell_size=GRID_CELL_SIZE_M,\n",
    "        height=height,\n",
    "        width=width,\n",
    "    )\n",
    "    fixed_code = np.where(inside, fixed_code, 0).astype(np.uint8)\n",
    "    fixed_mask = fixed_code > 0\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    counts = np.zeros((height, width, 5), dtype=np.uint16)\n",
    "\n",
    "    # Keep some realizations for the panel\n",
    "    panel_n = max(1, int(PANEL_N))\n",
    "    if N_REALIZATIONS <= panel_n:\n",
    "        keep_ids = list(range(1, N_REALIZATIONS + 1))\n",
    "    else:\n",
    "        keep_ids = np.linspace(1, N_REALIZATIONS, panel_n, dtype=int).tolist()\n",
    "        keep_ids = sorted(set(keep_ids))\n",
    "\n",
    "    kept_realizations: List[np.ndarray] = []\n",
    "\n",
    "    for r_idx in range(1, N_REALIZATIONS + 1):\n",
    "        sh = SpatialHash(xmin=xmin, ymin=ymin, cell_size_m=max(search_radius, EPS))\n",
    "        sh.add_many(unit_data.x, unit_data.y, unit_data.code)\n",
    "\n",
    "        cat = np.zeros((height, width), dtype=np.uint8)\n",
    "        cat[fixed_mask] = fixed_code[fixed_mask]\n",
    "\n",
    "        sim_lin = inside_lin[~fixed_mask.ravel()[inside_lin]]\n",
    "        path = sim_lin.copy()\n",
    "        rng.shuffle(path)\n",
    "\n",
    "        for lin in path:\n",
    "            row = lin // width\n",
    "            col = lin % width\n",
    "            x0 = float(grid_x[row, col])\n",
    "            y0 = float(grid_y[row, col])\n",
    "\n",
    "            neigh = sh.query_radius(x0, y0, radius_m=search_radius, max_neighbors=MAX_NEIGHBORS)\n",
    "\n",
    "            if len(neigh) < MIN_NEIGHBORS:\n",
    "                probs = marginal_p.copy()\n",
    "            else:\n",
    "                w = ok_weights(\n",
    "                    cond_pts=neigh,\n",
    "                    x0=x0,\n",
    "                    y0=y0,\n",
    "                    model_type=variogram.model_type,\n",
    "                    range_m=variogram.range_m,\n",
    "                    nugget=variogram.nugget,\n",
    "                    partial_sill_ref=partial_ref,\n",
    "                )\n",
    "                probs = conditional_probs_sis(\n",
    "                    cond_pts=neigh,\n",
    "                    weights=w,\n",
    "                    present_codes=unit_data.present_codes,\n",
    "                    marginal_p=marginal_p,\n",
    "                    partial_sills=partial_sills,\n",
    "                    nugget=variogram.nugget,\n",
    "                )\n",
    "\n",
    "            probs = np.clip(probs, 0.0, 1.0)\n",
    "            s = float(np.sum(probs))\n",
    "            probs = probs / s if (s > 0.0 and np.isfinite(s)) else marginal_p.copy()\n",
    "\n",
    "            code = int(rng.choice(np.arange(1, 6), p=probs))\n",
    "            cat[row, col] = np.uint8(code)\n",
    "            sh.add_point(CondPoint(x=x0, y=y0, code=code))\n",
    "\n",
    "        for c in range(1, 6):\n",
    "            counts[:, :, c - 1] += ((cat == c) & inside).astype(np.uint16)\n",
    "\n",
    "        if r_idx in keep_ids:\n",
    "            kept_realizations.append(cat.copy())\n",
    "\n",
    "        print(f\"Unit {unit_data.unit}: realization {r_idx}/{N_REALIZATIONS} complete\")\n",
    "\n",
    "    prob_stack = counts.astype(np.float32) / float(N_REALIZATIONS)\n",
    "    prob_stack[~inside, :] = np.nan\n",
    "\n",
    "    # SAFE p_max and winner (no RuntimeWarning)\n",
    "    p_max, winner, all_nan = safe_max_and_argmax(prob_stack)\n",
    "\n",
    "    # Uncertainty threshold -> label uncertain as 0\n",
    "    uncertain_mask = (p_max < float(UNCERTAIN_TOLERANCE)) | ~np.isfinite(p_max)\n",
    "    winner[uncertain_mask] = 0\n",
    "\n",
    "    # MMU cleanup (only affects categorical)\n",
    "    if MMU_CELLS and int(MMU_CELLS) > 1:\n",
    "        winner = mmu_cleanup(winner, mmu_cells=int(MMU_CELLS)).astype(np.uint8)\n",
    "\n",
    "    # Write rasters\n",
    "    for i, cls in enumerate(CLASSES):\n",
    "        write_geotiff(\n",
    "            unit_out / f\"prob_{safe_name(cls)}.tif\",\n",
    "            prob_stack[:, :, i].astype(np.float32),\n",
    "            transform=transform,\n",
    "            crs_epsg=CRS_EPSG,\n",
    "            nodata=np.nan,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "    write_geotiff(\n",
    "        unit_out / \"p_max_confidence.tif\",\n",
    "        p_max.astype(np.float32),\n",
    "        transform=transform,\n",
    "        crs_epsg=CRS_EPSG,\n",
    "        nodata=np.nan,\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "\n",
    "    write_geotiff(\n",
    "        unit_out / \"categorical_winner.tif\",\n",
    "        winner.astype(np.uint8),\n",
    "        transform=transform,\n",
    "        crs_epsg=CRS_EPSG,\n",
    "        nodata=0,\n",
    "        dtype=\"uint8\",\n",
    "    )\n",
    "\n",
    "    # Realizations panel PNG (diagnostic #5)\n",
    "    plot_realizations_panel_png(\n",
    "        out_png=unit_out / \"realizations_panel.png\",\n",
    "        realizations=kept_realizations,\n",
    "        inside=inside,\n",
    "        title=f\"{unit_data.unit} - SIS realizations (panel n={len(kept_realizations)} of {N_REALIZATIONS})\",\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        \"strManagementUnit\": unit_data.unit,\n",
    "        \"n_points\": int(len(unit_data.x)),\n",
    "        \"present_classes\": [CODE_TO_CLASS[c] for c in unit_data.present_codes],\n",
    "        \"grid_cell_size_m\": float(GRID_CELL_SIZE_M),\n",
    "        \"extent_buffer_m\": float(EXTENT_BUFFER_M),\n",
    "        \"use_point_envelope_mask\": bool(USE_POINT_ENVELOPE_MASK),\n",
    "        \"envelope_buffer_m\": float(ENVELOPE_BUFFER_M),\n",
    "        \"variogram_selected\": {\n",
    "            \"model_type\": variogram.model_type,\n",
    "            \"range_m\": float(variogram.range_m),\n",
    "            \"nugget\": float(variogram.nugget),\n",
    "            \"sill_total_J\": float(variogram.sill_total),\n",
    "        },\n",
    "        \"search_radius_m\": float(search_radius),\n",
    "        \"min_neighbors\": int(MIN_NEIGHBORS),\n",
    "        \"max_neighbors\": int(MAX_NEIGHBORS),\n",
    "        \"n_realizations\": int(N_REALIZATIONS),\n",
    "        \"random_seed\": int(seed),\n",
    "        \"uncertain_tolerance\": float(UNCERTAIN_TOLERANCE),\n",
    "        \"mmu_cells\": int(MMU_CELLS),\n",
    "        \"class_to_code\": CLASS_TO_CODE,\n",
    "        \"code_to_class\": CODE_TO_CLASS,\n",
    "    }\n",
    "    (unit_out / \"metadata.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    return {\n",
    "        \"strManagementUnit\": unit_data.unit,\n",
    "        \"n_points\": int(len(unit_data.x)),\n",
    "        \"variogram_type\": variogram.model_type,\n",
    "        \"range_m\": float(variogram.range_m),\n",
    "        \"nugget\": float(variogram.nugget),\n",
    "        \"search_radius_m\": float(search_radius),\n",
    "        \"n_realizations\": int(N_REALIZATIONS),\n",
    "        \"status\": \"ok\",\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Orchestrator: per unit, diagnostics + SIS\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def run_sis_workflow(input_csv: str, output_dir: str, seed: int = RANDOM_SEED) -> pd.DataFrame:\n",
    "    out_dir = Path(output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = read_points(input_csv)\n",
    "    units = split_by_unit(df)\n",
    "\n",
    "    rows: List[Dict[str, object]] = []\n",
    "\n",
    "    for u in units:\n",
    "        unit_out = out_dir / safe_name(u.unit)\n",
    "        print(\"\\n\" + \"=\" * 72)\n",
    "        print_unit_class_summary(u)\n",
    "\n",
    "        # Select shared variogram by block CV + CV table for diagnostics\n",
    "        best_model, cv_df = select_shared_variogram_by_block_cv(u, seed=seed)\n",
    "        print(\n",
    "            f\"Selected variogram: {best_model.model_type} \"\n",
    "            f\"(range={best_model.range_m:.2f} m, nugget={best_model.nugget:.4f}, sill_J={best_model.sill_total:.4f})\"\n",
    "        )\n",
    "\n",
    "        # Diagnostics 1,2,4\n",
    "        write_unit_variogram_diagnostics(\n",
    "            unit_data=u,\n",
    "            unit_out=unit_out,\n",
    "            best_model=best_model,\n",
    "            cv_df=cv_df,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # SIS + outputs + diagnostic 5 (realizations panel)\n",
    "        row = run_sis_for_unit(\n",
    "            unit_data=u,\n",
    "            out_dir=out_dir,\n",
    "            variogram=best_model,\n",
    "            seed=seed,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    summary = pd.DataFrame(rows).sort_values(\"strManagementUnit\").reset_index(drop=True)\n",
    "    summary_path = out_dir / \"sis_summary_by_unit.csv\"\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    print(f\"\\nWrote: {summary_path}\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Run (execute in notebook)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "summary_df = run_sis_workflow(INPUT_CSV, OUTPUT_DIR, seed=RANDOM_SEED)\n",
    "summary_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
